{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaleOv-JRzYB",
        "outputId": "cd66b012-6934-417f-b768-4a8ac8d0c822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-10 14:12:43--  https://nlp.cs.washington.edu/ambigqa/data/nqopen.zip\n",
            "Resolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.120, 2607:4000:200:12::78\n",
            "Connecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4081107 (3.9M) [application/zip]\n",
            "Saving to: ‘data/nqopen.zip’\n",
            "\n",
            "data/nqopen.zip     100%[===================>]   3.89M  7.49MB/s    in 0.5s    \n",
            "\n",
            "2023-06-10 14:12:44 (7.49 MB/s) - ‘data/nqopen.zip’ saved [4081107/4081107]\n",
            "\n",
            "Archive:  data/nqopen.zip\n",
            "  inflating: data/LICENSE            \n",
            "  inflating: data/nqopen-train.json  \n",
            "  inflating: data/nqopen-dev.json    \n",
            "  inflating: data/nqopen-test.json   \n"
          ]
        }
      ],
      "source": [
        "#!/bin/sh\n",
        "\n",
        "!mkdir -p data\n",
        "!wget https://nlp.cs.washington.edu/ambigqa/data/nqopen.zip -O data/nqopen.zip\n",
        "!unzip -d data data/nqopen.zip\n",
        "!rm data/nqopen.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXXt-OJaKd2D",
        "outputId": "ac9a7356-b2b7-47cf-a33f-45b45a3be8d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'xlm-roberta-large-vi-qa'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 33\u001b[K\n",
            "Unpacking objects: 100% (33/33), 4.40 KiB | 644.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/ancs21/xlm-roberta-large-vi-qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0k3jBt_RVGI",
        "outputId": "0ac821d0-54d0-4d44-b3b0-3daf4ad036d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'bart-large-finetuned-squadv1'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 29 (delta 9), reused 29 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (29/29), 538.55 KiB | 7.80 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 3.02 GiB | 58.11 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/valhalla/bart-large-finetuned-squadv1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFClEMVgOdOx",
        "outputId": "ad80381c-0d19-4d6b-efd1-359de428e41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'bloom-560m-pre-train-v2-qa-l6-l2'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 21 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), 3.32 KiB | 851.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/sai1881/bloom-560m-pre-train-v2-qa-l6-l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5Pz1HGaGT5b",
        "outputId": "39efa713-0ca5-4db4-ddef-d6f2a1a69e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlMGMC_dGXpN",
        "outputId": "b40c5caa-4bff-4cda-be0d-de5b9b1add5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')  # Dữ liệu cho tokenizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_7EpoVXONrx",
        "outputId": "02b14c7d-4011-447d-9fd8-5f36c8a38181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.3.0\n",
            "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==2.3.0) (1.22.4)\n",
            "Collecting boto3 (from transformers==2.3.0)\n",
            "  Downloading boto3-1.26.151-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==2.3.0) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from transformers==2.3.0) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==2.3.0) (2022.10.31)\n",
            "Collecting sentencepiece (from transformers==2.3.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses (from transformers==2.3.0)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.30.0,>=1.29.151 (from boto3->transformers==2.3.0)\n",
            "  Downloading botocore-1.29.151-py3-none-any.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->transformers==2.3.0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->transformers==2.3.0)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.3.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.3.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.3.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.3.0) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.3.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.3.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.3.0) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.151->boto3->transformers==2.3.0) (2.8.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=358cffaef19bff47e150b6659a1200b33f298a450638dc5d0dae65d0dcb7ee8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3, transformers\n",
            "Successfully installed boto3-1.26.151 botocore-1.29.151 jmespath-1.0.1 s3transfer-0.6.1 sacremoses-0.0.53 sentencepiece-0.1.99 transformers-2.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tqdm==4.32.2\n",
            "  Downloading tqdm-4.32.2-py2.py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.14.4 requires tqdm>=4.48.0, but you have tqdm 4.32.2 which is incompatible.\n",
            "prophet 1.1.3 requires tqdm>=4.36.1, but you have tqdm 4.32.2 which is incompatible.\n",
            "spacy 3.5.2 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.32.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tqdm-4.32.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX==1.9\n",
            "  Downloading tensorboardX-1.9-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.7/190.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.9) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.9) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.9) (1.16.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines==1.2.0) (1.16.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses==0.0.41\n",
            "  Downloading sacremoses-0.0.41.tar.gz (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.5/883.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.41) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.41) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.41) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.41) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.41) (4.32.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-py3-none-any.whl size=893303 sha256=8c2da46d32380b33d77c465346593c6987a228b9da4760fc1d9df7f7c0bca556\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/1a/8b/64a85bd4f0b31f53d1fa8e27f99bed57642ef8d10fc67bee17\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "  Attempting uninstall: sacremoses\n",
            "    Found existing installation: sacremoses 0.0.53\n",
            "    Uninstalling sacremoses-0.0.53:\n",
            "      Successfully uninstalled sacremoses-0.0.53\n",
            "Successfully installed sacremoses-0.0.41\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645933 sha256=aae7f23de728574bc5d628ed09bd636f5758b648e3dd890285b38244c42a48fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-6.2.0-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.3)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.32.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.27.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0)\n",
            "Collecting underthesea-core==1.0.0 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.0-cp310-cp310-manylinux2010_x86_64.whl (599 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.6/599.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2022.10.31)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.9 underthesea-6.2.0 underthesea-core-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==2.3.0\n",
        "#!pip install numpy==1.16.4\n",
        "!pip install tqdm==4.32.2\n",
        "!pip install tensorboardX==1.9\n",
        "!pip install jsonlines==1.2.0\n",
        "!pip install sacremoses==0.0.41\n",
        "!pip install jsonlines\n",
        "!pip install vncorenlp\n",
        "!pip install underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpdZOKa7NBnq"
      },
      "outputs": [],
      "source": [
        "!pip install tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZqV8A4RzQqP",
        "outputId": "b3129d6f-3fbe-46f3-dd16-109d31b86e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 143844, done.\u001b[K\n",
            "remote: Counting objects: 100% (1358/1358), done.\u001b[K\n",
            "remote: Compressing objects: 100% (589/589), done.\u001b[K\n",
            "remote: Total 143844 (delta 815), reused 1092 (delta 672), pack-reused 142486\u001b[K\n",
            "Receiving objects: 100% (143844/143844), 143.19 MiB | 23.34 MiB/s, done.\n",
            "Resolving deltas: 100% (107532/107532), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDd6k1eqJ5Xr"
      },
      "source": [
        "# dùng token xlm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmcWnSLK3SWx"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertTokenizer\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-xlarge-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ymu8MyKJ74K"
      },
      "source": [
        "# lấy ra từng span tạo nên câu trả lời và vị trí của span đó trong đoạn văn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTguUAiYhvrr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "def get_pos_span(passage, answer,query_id):\n",
        "  # lấy vị trí từng từ tạo nên câu trả lời nằm trong đoạn văn\n",
        "      numbers=[]\n",
        "      list_loi=[]\n",
        "      for i in answer:\n",
        "          if i in passage:\n",
        "            numbers.append(passage.index(i))\n",
        "          else:\n",
        "            print(\"lỗi tại id:   \", query_id)\n",
        "      A = []\n",
        "      sub_array = [numbers[0]]\n",
        "     # lấy ra vị trí bắt đầu và kết thúc của từng từ đó\n",
        "      for i in range(1, len(numbers)):\n",
        "          if numbers[i] - numbers[i-1] == 1:\n",
        "              sub_array.append(numbers[i])\n",
        "          elif numbers[i]-numbers[i-1]==0:\n",
        "              sub_array.append(numbers[i]+1)\n",
        "          elif numbers[i]-sub_array[len(sub_array)-1]==1:\n",
        "              sub_array.append(numbers[i])\n",
        "          else:\n",
        "              A.append(sub_array)\n",
        "              sub_array = [numbers[i]]\n",
        "\n",
        "      A.append(sub_array)\n",
        "      # lấy ra vị trí bắt đầu và kết thúc của chuỗi dài nhất\n",
        "      pos=[]\n",
        "      for i in A:\n",
        "          if (len (i)==1):\n",
        "              pos.append((i[0],i[0]))\n",
        "          else:\n",
        "              pos.append((np.min(i), np.max(i)))\n",
        "   # lấy ra từng span của chuỗi dài nhất\n",
        "      all_span=[]\n",
        "      for i in pos:\n",
        "          all_span.append(\" \".join(passage[np.min(i):np.max(i)+1]))\n",
        "      for i in range(len(all_span)):\n",
        "        all_span[i]=re.sub(r'\\s+([^\\w\\s])', r'\\1', all_span[i])\n",
        "\n",
        "      return all_span, pos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0Y8YrJGxVUb"
      },
      "outputs": [],
      "source": [
        "def annotation(example, tokenizer):\n",
        "  query_id=example['query_id']\n",
        "  answer=example['Answer_new']\n",
        "  #print(answer)\n",
        "  answer_token=word_tokenize(answer)\n",
        "  #print(answer_token)\n",
        "  passage=example['passage']\n",
        "  p_tokens=word_tokenize(passage)\n",
        "  query=example['question']\n",
        "  q_tokens=word_tokenize(query)\n",
        "  qp_tokens= [tokenizer.cls_token] + q_tokens + [tokenizer.sep_token] + p_tokens + [tokenizer.sep_token]\n",
        "  all_span,pos=get_pos_span(qp_tokens, answer_token, query_id)\n",
        "  return query_id, all_span, pos, qp_tokens, answer, query, passage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn7E5Mt4u9C6"
      },
      "outputs": [],
      "source": [
        "# đọc file train ở dạng jsonlike\n",
        "import jsonlines\n",
        "# Read examples\n",
        "all_example = []\n",
        "with jsonlines.open(\"/content/drive/MyDrive/DS204/Notebook/musst/data_29_5_jsonline_10_6.jsonl\") as reader:\n",
        "  for example in reader:\n",
        "    all_example.append(example)\n",
        "print(\"The original size of dataset is \", len(all_example))\n",
        "\n",
        "# lấy ra mảng chứa thông tin\n",
        "all_ans_span_pos=[]\n",
        "for example in all_example:\n",
        "     print(\"id     \", example['query_id'],\"\\n_________ \\n answer: \",example['Answer_new'])\n",
        "     all_ans_span_pos.append(annotation(example, tokenizer))\n",
        "\n",
        "# tạo dict chứa thông tin\n",
        "all_annoted_spans = {}\n",
        "for (query_id, all_span, pos, qp_text_tokens, answer, query, passage) in all_ans_span_pos:\n",
        "  result = {}\n",
        "  result['annoted_spans_pos'] = pos\n",
        "  result['annoted_spans_text'] =all_span\n",
        "  all_span_i=\" \".join(all_span[i] for i in range(len(all_span)))\n",
        "  all_spanj=re.sub(r'\\s+([^\\w\\s])', r'\\1', all_span_i)\n",
        "  result['ant_answer'] = all_spanj\n",
        "  result['raw_answer'] = answer\n",
        "  #result['edit_distance'] = editdistance.eval(result['ant_answer'], result['raw_answer'])\n",
        "  result['query'] = query\n",
        "  result['passage'] = passage\n",
        "  all_annoted_spans[query_id] = result\n",
        "# in ra dict\n",
        "all_annoted_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HUXf8cRvH47"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Hàm trung gian để chuyển đổi int64 thành int\n",
        "def convert_int64_to_int(obj):\n",
        "    if isinstance(obj, np.int64):\n",
        "        return int(obj)\n",
        "    return obj\n",
        "\n",
        "# Ghi dữ liệu vào file JSON với hàm trung gian\n",
        "with open(\"/content/drive/MyDrive/DS204/Notebook/musst/train_span_annotation.json\", mode='w') as writer:\n",
        "    json.dump(all_annoted_spans, writer, indent=4, default=convert_int64_to_int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y69DIxgBsOod",
        "outputId": "28522334-51fb-4a22-eb7a-39a466cb1691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng mẫu trong tập train: 292\n",
            "Số lượng mẫu trong tập test: 33\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Đường dẫn đến file JSON\n",
        "json_file_path = \"/content/drive/MyDrive/DS204/Notebook/musst/train_span_annotation.json\"\n",
        "\n",
        "# Tỷ lệ chia tập train và test (vd: 80% train, 20% test)\n",
        "train_ratio = 0.9\n",
        "\n",
        "# Đọc dữ liệu từ file JSON\n",
        "with open(json_file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Chuyển đổi dữ liệu thành danh sách\n",
        "data = list(data)\n",
        "\n",
        "# Xáo trộn dữ liệu\n",
        "try:\n",
        "    random.shuffle(data)\n",
        "except KeyError as e:\n",
        "    print(\"Lỗi KeyError:\", e)\n",
        "    print(\"Có phần tử không hợp lệ trong dữ liệu.\")\n",
        "\n",
        "# Tính chỉ số chia tập\n",
        "split_index = int(len(data) * train_ratio)\n",
        "\n",
        "# Chia tập dữ liệu thành train và test\n",
        "train_data = data[:split_index]\n",
        "test_data = data[split_index:]\n",
        "\n",
        "# In số lượng mẫu trong mỗi tập\n",
        "print(\"Số lượng mẫu trong tập train:\", len(train_data))\n",
        "print(\"Số lượng mẫu trong tập test:\", len(test_data))\n",
        "\n",
        "# Lưu tập train vào file train.json\n",
        "with open(\"/content/drive/MyDrive/DS204/Notebook/musst/train.json\", 'w') as f:\n",
        "    json.dump(train_data, f)\n",
        "\n",
        "# Lưu tập test vào file test.json\n",
        "with open(\"/content/drive/MyDrive/DS204/Notebook/musst/test.json\", 'w') as f:\n",
        "    json.dump(test_data, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "urPC-REVVQRg",
        "outputId": "b1d0059c-0087-4997-e4cf-8978d838191e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-10 15:27:43.348910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "06/10/2023 15:27:52 - WARNING - __main__ -   Device: cuda, n_gpu: 1\n",
            "06/10/2023 15:27:52 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-config.json from cache at /root/.cache/torch/transformers/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.044d033374cf383119201e83a30fc592581d33c3833ac0675f55b2765509ce4e\n",
            "06/10/2023 15:27:52 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "06/10/2023 15:27:52 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v2-spiece.model from cache at /root/.cache/torch/transformers/02112eba687f794948810d2215028e9a0e77585b966ac59854a8d73e2d344d0b.c81d4deb77aec08ce575b7a39a989a79dd54f321bfb82c2b54dd35f52f8182cf\n",
            "06/10/2023 15:27:52 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-pytorch_model.bin from cache at /root/.cache/torch/transformers/e6fd1be612c6797d1dc556e26724c9ed6ea0a45b79e3486533eccf9432a23ce6.857dc1dec4c7a7f2a7a0088f4139c2424ecc3d9a238f5c02de82f24a1eb69439\n",
            "06/10/2023 15:27:53 - INFO - transformers.modeling_utils -   Weights of MUSTTransformerModel not initialized from pretrained model: ['span_outputs.0.weight', 'span_outputs.0.bias', 'span_outputs.1.weight', 'span_outputs.1.bias', 'span_outputs.2.weight', 'span_outputs.2.bias', 'span_outputs.3.weight', 'span_outputs.3.bias', 'span_outputs.4.weight', 'span_outputs.4.bias', 'span_outputs.5.weight', 'span_outputs.5.bias', 'span_outputs.6.weight', 'span_outputs.6.bias', 'span_outputs.7.weight', 'span_outputs.7.bias', 'span_outputs.8.weight', 'span_outputs.8.bias', 'span_outputs.9.weight', 'span_outputs.9.bias', 'span_outputs.10.weight', 'span_outputs.10.bias', 'span_outputs.11.weight', 'span_outputs.11.bias', 'span_outputs.12.weight', 'span_outputs.12.bias', 'span_outputs.13.weight', 'span_outputs.13.bias', 'span_outputs.14.weight', 'span_outputs.14.bias', 'span_outputs.15.weight', 'span_outputs.15.bias', 'span_outputs.16.weight', 'span_outputs.16.bias', 'span_outputs.17.weight', 'span_outputs.17.bias', 'span_outputs.18.weight', 'span_outputs.18.bias', 'span_outputs.19.weight', 'span_outputs.19.bias', 'span_outputs.20.weight', 'span_outputs.20.bias', 'span_outputs.21.weight', 'span_outputs.21.bias', 'span_outputs.22.weight', 'span_outputs.22.bias', 'span_outputs.23.weight', 'span_outputs.23.bias', 'span_outputs.24.weight', 'span_outputs.24.bias', 'span_outputs.25.weight', 'span_outputs.25.bias', 'span_outputs.26.weight', 'span_outputs.26.bias', 'span_outputs.27.weight', 'span_outputs.27.bias', 'span_outputs.28.weight', 'span_outputs.28.bias', 'span_outputs.29.weight', 'span_outputs.29.bias', 'span_outputs.30.weight', 'span_outputs.30.bias', 'span_outputs.31.weight', 'span_outputs.31.bias', 'span_outputs.32.weight', 'span_outputs.32.bias', 'span_outputs.33.weight', 'span_outputs.33.bias', 'span_outputs.34.weight', 'span_outputs.34.bias', 'span_outputs.35.weight', 'span_outputs.35.bias', 'span_outputs.36.weight', 'span_outputs.36.bias', 'span_outputs.37.weight', 'span_outputs.37.bias', 'span_outputs.38.weight', 'span_outputs.38.bias', 'span_outputs.39.weight', 'span_outputs.39.bias', 'span_outputs.40.weight', 'span_outputs.40.bias', 'span_outputs.41.weight', 'span_outputs.41.bias', 'span_outputs.42.weight', 'span_outputs.42.bias', 'span_outputs.43.weight', 'span_outputs.43.bias', 'span_outputs.44.weight', 'span_outputs.44.bias', 'span_outputs.45.weight', 'span_outputs.45.bias', 'span_outputs.46.weight', 'span_outputs.46.bias', 'span_outputs.47.weight', 'span_outputs.47.bias', 'span_outputs.48.weight', 'span_outputs.48.bias', 'span_outputs.49.weight', 'span_outputs.49.bias', 'span_outputs.50.weight', 'span_outputs.50.bias', 'span_outputs.51.weight', 'span_outputs.51.bias', 'span_outputs.52.weight', 'span_outputs.52.bias', 'span_outputs.53.weight', 'span_outputs.53.bias', 'span_outputs.54.weight', 'span_outputs.54.bias', 'span_outputs.55.weight', 'span_outputs.55.bias', 'span_outputs.56.weight', 'span_outputs.56.bias', 'span_outputs.57.weight', 'span_outputs.57.bias', 'span_outputs.58.weight', 'span_outputs.58.bias', 'span_outputs.59.weight', 'span_outputs.59.bias', 'span_outputs.60.weight', 'span_outputs.60.bias']\n",
            "06/10/2023 15:27:53 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in MUSTTransformerModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
            "06/10/2023 15:27:53 - INFO - __main__ -   Training/evaluation/prediction hyperparameters Namespace(do_train=True, do_eval=False, do_pred=True, evaluate_during_training=True, data_dir='/content/drive/MyDrive/DS204/Notebook/musst/', train_file='data_29_5_jsonline_10_6.jsonl', eval_file='dev.jsonl', pred_file='data_29_5_jsonline_10_6.jsonl', span_annotation_file='/content/drive/MyDrive/DS204/Notebook/musst/train_span_annotation.json', reference_file='dev_ref.json', output_dir='/content/drive/MyDrive/DS204/Notebook/musst/', overwrite_output_dir=True, model_name_or_path='albert-large-v2', config_name='', tokenizer_name='', max_num_spans=60, ed_threshold=500, max_seq_len=512, per_gpu_train_batch_size=16, per_gpu_pred_batch_size=16, learning_rate=5e-05, weight_decay=0.1, adam_epsilon=1e-06, adam_beta1=0.9, adam_beta2=0.999, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_rate=0.0, logging_steps=100, eval_steps=50, no_cuda=False, seed=1996, n_gpu=1, device=device(type='cuda'))\n",
            "The original dataset size is 325\n",
            "The size of dataset with annotations is 325\n",
            "The size of dataset with examples (Edit distance <= 500) is 325\n",
            "1 : 3\n",
            "2 : 43\n",
            "3 : 57\n",
            "4 : 37\n",
            "5 : 30\n",
            "6 : 23\n",
            "7 : 15\n",
            "8 : 22\n",
            "9 : 7\n",
            "10 : 12\n",
            "11 : 9\n",
            "12 : 8\n",
            "13 : 11\n",
            "14 : 8\n",
            "15 : 8\n",
            "16 : 3\n",
            "17 : 9\n",
            "18 : 1\n",
            "19 : 1\n",
            "20 : 2\n",
            "22 : 2\n",
            "23 : 2\n",
            "26 : 2\n",
            "28 : 1\n",
            "29 : 3\n",
            "30 : 4\n",
            "33 : 1\n",
            "35 : 1\n",
            "The size of dataset with examples (0 < Span number) is 325\n",
            "06/10/2023 15:27:53 - INFO - __main__ -   ***** Running training *****\n",
            "06/10/2023 15:27:53 - INFO - __main__ -     Num examples = 325\n",
            "06/10/2023 15:27:53 - INFO - __main__ -     Num Epochs = 3\n",
            "06/10/2023 15:27:53 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
            "06/10/2023 15:27:53 - INFO - __main__ -     Total train batch size (w. parallel) = 16\n",
            "06/10/2023 15:27:53 - INFO - __main__ -     Total optimization steps = 63\n",
            "  0% 0/21 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DS204/Notebook/musst/qa_multi_span/main.py\", line 235, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/DS204/Notebook/musst/qa_multi_span/main.py\", line 221, in main\n",
            "    global_step, tr_loss = train(args, model, tokenizer, logger)\n",
            "  File \"/content/drive/MyDrive/DS204/Notebook/musst/qa_multi_span/train.py\", line 101, in train\n",
            "    outputs = model(input_ids=input_ids,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/DS204/Notebook/musst/qa_multi_span/model.py\", line 30, in forward\n",
            "    outputs = self.albert(input_ids=input_ids,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_albert.py\", line 537, in forward\n",
            "    encoder_outputs = self.encoder(embedding_output,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_albert.py\", line 331, in forward\n",
            "    layer_group_output = self.albert_layer_groups[group_idx](hidden_states, attention_mask, head_mask[group_idx*layers_per_group:(group_idx+1)*layers_per_group])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_albert.py\", line 286, in forward\n",
            "    layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_albert.py\", line 268, in forward\n",
            "    hidden_states = self.full_layer_layer_norm(ffn_output + attention_output[0])\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 14.75 GiB total capacity; 13.63 GiB already allocated; 26.81 MiB free; 13.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/DS204/Notebook/musst/qa_multi_span/main.py \\\n",
        "    --model_name_or_path=\"albert-large-v2\" \\\n",
        "    --do_train \\\n",
        "    --do_pred\\\n",
        "    --train_file=\"data_29_5_jsonline_10_6.jsonl\"\\\n",
        "    --data_dir=\"/content/drive/MyDrive/DS204/Notebook/musst/\"\\\n",
        "    --output_dir=\"/content/drive/MyDrive/DS204/Notebook/musst/qa_multi_span/\" \\\n",
        "    --span_annotation_file=\"/content/drive/MyDrive/DS204/Notebook/musst/train_span_annotation.json\" \\\n",
        "    --overwrite_output_dir  \\\n",
        "    --pred_file=\"data_29_5_jsonline_10_6.jsonl\"\\\n",
        "    --per_gpu_train_batch_size=4   \\\n",
        "    --per_gpu_pred_batch_size=4  \\\n",
        "    --num_train_epochs=3.0 \\\n",
        "    --learning_rate=5e-5 \\\n",
        "    --evaluate_during_training \\\n",
        "    --eval_steps=50\\\n",
        "    --max_seq_len=512 \\\n",
        "    --max_num_spans=60 \\\n",
        "    --ed_threshold=500\\\n",
        "    --seed=1996\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcXXIB8sTsOe",
        "outputId": "7d5ac039-ac13-46b4-dd82-6b9132058f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wok8Y2tQNcGk"
      },
      "outputs": [],
      "source": [
        "python /content/transformers/examples/pytorch/question-answering/run_qa_beam_search.py \\\n",
        "    --model_name_or_path xlnet-large-cased \\\n",
        "    --dataset_name squad \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir /content/drive/xlnet \\\n",
        "    --per_device_eval_batch_size=4  \\\n",
        "    --per_device_train_batch_size=4   \\\n",
        "    --save_steps 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRSXgygUPYC7",
        "outputId": "488a9a71-0674-436b-947a-146a1ba97bd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DS204/Notebook\n",
            "Cloning into 'MultiSpanQA'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 64 (delta 32), reused 51 (delta 19), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (64/64), 11.44 MiB | 2.37 MiB/s, done.\n",
            "Updating files: 100% (17/17), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/DS204/Notebook\n",
        "!git clone https://github.com/haonan-li/MultiSpanQA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cul83Bz2aeg",
        "outputId": "76fb447f-dc8a-4179-8278-c2b3a4d89039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Could not find or load main class edu.stanford.nlp.pipeline.StanfordCoreNLPServer\n",
            "Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.pipeline.StanfordCoreNLPServer\n"
          ]
        }
      ],
      "source": [
        "!java -mx20g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n",
        "    -preload tokenize,ssplit,pos,parse \\\n",
        "    -port 8889"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9USepaWKgB_H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}